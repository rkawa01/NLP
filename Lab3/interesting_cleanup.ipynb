{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HUGGINGFACE_HUB_TOKEN'])  \n",
    "ds_stream = load_dataset(\"uonlp/CulturaX\", \"pl\", split=\"train\", streaming=True)\n",
    "ds_shuffled = ds_stream.shuffle(seed=42, buffer_size=10_000)\n",
    "\n",
    "data_list = []\n",
    "for i, item in enumerate(ds_shuffled):\n",
    "    if i >= 50_000:\n",
    "        break\n",
    "    data_list.append(item)\n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"Loaded {i}/50,000...\")\n",
    "\n",
    "ds_small = Dataset.from_list(data_list)\n",
    "\n",
    "print(f\"Created Dataset with {len(ds_small)} samples\")\n",
    "print(f\"Type: {type(ds_small)}\")\n",
    "\n",
    "print(ds_small[0])\n",
    "print(len(ds_small))\n",
    "print(ds_small.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95102594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_function(example):\n",
    "    import re\n",
    "    text = example['text']\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # HTML\n",
    "    text = re.sub(r'https?://\\S+', ' ', text) # Linki\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Bia≈Çe znaki\n",
    "    example['text'] = text\n",
    "    return example\n",
    "\n",
    "# --- 2. NOWA, BARDZIEJ AGRESYWNA KLASA QualityFilter (Poziom 4) ---\n",
    "class QualityFilter:\n",
    "    def __init__(self):\n",
    "        import re \n",
    "        \n",
    "        self.MIN_TEXT_LENGTH = 100\n",
    "        self.MAX_TEXT_LENGTH = 50_000\n",
    "\n",
    "        # Heurystyki jƒôzykowe (bez zmian)\n",
    "        self.COMMON_POLISH = set(\n",
    "            \"w i z na siƒô ≈ºe do nie o po to co ma ale jest jak od \"\n",
    "            \"do w tym za o siƒô ale z to i na nie ma po co jak \"\n",
    "            \"jest od ≈ºe ten by≈Ç mi lub tak\".split()\n",
    "        )\n",
    "        self.COMMON_ENGLISH = set(\n",
    "            \"the be to of and a in that have i it for not on with \"\n",
    "            \"he as you do at this but his by from they we say her \"\n",
    "            \"she or an will my one all would there their what so \"\n",
    "            \"up out if about who get which go me\".split()\n",
    "        )\n",
    "\n",
    "        # --- NOWE, AGRESYWNE REGEXY ---\n",
    "        \n",
    "        # Wzorzec na E-commerce, Og≈Çoszenia, Ceny\n",
    "        self.ecommerce_pattern = re.compile(\n",
    "            r'(\\b(z≈Ç|PLN|cenie|okazja|SALE|promocja|rabat(u|em)?)\\b.*){2,}|' + # Obni≈ºony pr√≥g do {2,}\n",
    "            r'(\\b(Kod produktu|Kod EAN|Cena netto:|Rozmiar:|Waga:|z≈Ç / m¬≤|Szafa\\.pl|Gandalf\\.com|Gomez\\.pl|Mivo\\.pl|KRN\\.pl|Morizon\\.pl|Gumtree)\\b)|' +\n",
    "            r'(zobacz oferty z \\d+ ksiƒôgarni)|' +\n",
    "            r'\\b(OSZCZƒòDZASZ \\d+ Z≈Å)\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        # Wzorzec na Listy, Agregatory, SEO, Tagi\n",
    "        self.list_seo_pattern = re.compile(\n",
    "            r'(\\s*\\|.*){5,}|' + \n",
    "            r'(^[\\s‚Ä¢*-].*$){5,}|' + \n",
    "            r'(^((\\d+\\.)|\\w\\))\\s.*$){5,}|' + \n",
    "            r'(üì¢.*){3,}|' +\n",
    "            r'(\\b(TAGI|Tags|Kategoria|Portal|Polecane wpisy|Najnowsze wpisy)\\b:?)|' +\n",
    "            r'(\\[podobne:)|(Czytaj wiƒôcej ¬ª)|(Program na dzi≈õ)|(Strona g≈Ç√≥wna ¬ª)',\n",
    "            re.MULTILINE | re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        # Wzorzec na Fora, Komentarze, B≈Çƒôdy Kodu\n",
    "        self.forum_errors_pattern = re.compile(\n",
    "            r'(\\b(Posted|napisa≈Ç|dnia|Autor:|dodany przez|komentarze:|kupi≈Ç/a|Zg≈Ço≈õ komentarz)\\b.*){2,}|' + # Obni≈ºony pr√≥g do {2,}\n",
    "            r'((\\d+ lat temu)|\\d{2}:\\d{2}:\\d{2})|' + \n",
    "            r'(\\b(Re: |Zobacz wƒÖtek)\\b)|' +\n",
    "            r'\\b(Warning: |failed to open stream|No such file or directory)\\b|' +\n",
    "            r'\\w{3} \\w{3} \\d{2}, \\d{4}', # np. Thu Jun 01, 2017\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Wzorzec na be≈Çkot i z≈Çe t≈Çumaczenia (wy≈Çapuje konkretne nonsensy)\n",
    "        self.garbled_pattern = re.compile(\n",
    "            r'\\b[bcdfghjklmnpqrstvwxzBCDFGHJKLMNPQRSTVWXZ]{7,}\\b|' +\n",
    "            r'\\b(Bulls Snap Up Improads|trasie Ro≈õlinƒô|≈õmierci pracy|stawaƒá w ogniach|za≈Çadowali w≈Çadzy)\\b',\n",
    "             re.IGNORECASE\n",
    "        )\n",
    "\n",
    "    def __call__(self, example):\n",
    "        text = example['text']\n",
    "        \n",
    "        if not (self.MIN_TEXT_LENGTH < len(text) < self.MAX_TEXT_LENGTH):\n",
    "            return False\n",
    "            \n",
    "        # Sprawdzamy nowe, agresywne filtry\n",
    "        if self.ecommerce_pattern.search(text):\n",
    "            return False\n",
    "        if self.list_seo_pattern.search(text):\n",
    "            return False\n",
    "        if self.forum_errors_pattern.search(text):\n",
    "            return False\n",
    "        if self.garbled_pattern.search(text):\n",
    "            return False\n",
    "            \n",
    "        # Filtr jƒôzykowy (bez zmian)\n",
    "        words = text.lower().split()\n",
    "        if not words: \n",
    "             return False\n",
    "             \n",
    "        polish_words = sum(1 for word in words if word in self.COMMON_POLISH)\n",
    "        english_words = sum(1 for word in words if word in self.COMMON_ENGLISH)\n",
    "        total_common_words = polish_words + english_words\n",
    "        \n",
    "        if total_common_words > 10: \n",
    "            if english_words / total_common_words > 0.15:\n",
    "                return False \n",
    "            if polish_words / len(words) < 0.05: \n",
    "                return False\n",
    "\n",
    "        return True # Pr√≥bka jest \"dobra\"\n",
    "\n",
    "def add_hash(example):\n",
    "    import hashlib\n",
    "    example['hash'] = hashlib.sha256(example['text'].encode('utf-8')).hexdigest()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b394da",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES_TO_USE = 12 \n",
    "\n",
    "print(f\"Rozmiar oryginalny (CulturaX sample): {len(ds_small)}\")\n",
    "print(f\"Uruchamiam przetwarzanie na {NUM_CORES_TO_USE} rdzeniach.\")\n",
    "\n",
    "print(\"Krok 1: Czyszczenie .map()...\")\n",
    "cleaned_ds = ds_small.map(\n",
    "    clean_text_function, \n",
    "    num_proc=NUM_CORES_TO_USE\n",
    ")\n",
    "\n",
    "print(\"Krok 2: Zaawansowane filtrowanie (Poziom 4) .filter()...\")\n",
    "quality_filter_instance_v4 = QualityFilter()\n",
    "filtered_ds = cleaned_ds.filter(\n",
    "    quality_filter_instance_v4, \n",
    "    num_proc=NUM_CORES_TO_USE\n",
    ")\n",
    "\n",
    "print(f\"Rozmiar po filtrowaniu jako≈õciowym: {len(filtered_ds)} (Usuniƒôto {len(cleaned_ds) - len(filtered_ds)})\")\n",
    "\n",
    "print(\"Krok 3: Obliczanie hashy do deduplikacji...\")\n",
    "ds_with_hashes = filtered_ds.map(\n",
    "    add_hash, \n",
    "    num_proc=NUM_CORES_TO_USE\n",
    ")\n",
    "\n",
    "hashes_seen = set()\n",
    "print(\"Krok 4: Deduplikacja .filter()...\")\n",
    "ds_deduplicated = ds_with_hashes.filter(\n",
    "    lambda x: (h := x['hash']) not in hashes_seen and not hashes_seen.add(h)\n",
    ")\n",
    "final_ds = ds_deduplicated.remove_columns(\"hash\")\n",
    "\n",
    "print(\"\\n--- WYNIK FINALNY (Poziom 4, Ultra-agresywny) ---\")\n",
    "print(f\"Rozmiar oryginalny: {len(ds_small)}\")\n",
    "print(f\"Rozmiar po czyszczeniu, filtrowaniu I deduplikacji: {len(final_ds)}\")\n",
    "print(f\"Ca≈Çkowita liczba usuniƒôtych pr√≥bek: {len(ds_small) - len(final_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- PR√ìBKI 100 PO ZAAWANSOWANYM CZYSZCZENIU ---\")\n",
    "\n",
    "for i, item in enumerate(final_ds):\n",
    "    if i >= 100:  # Zatrzymaj po pr√≥bce 100\n",
    "        break\n",
    "    print(f\"--- PR√ìBKA {i+1} ---\")\n",
    "    print(item['text'])\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a140d42",
   "metadata": {},
   "source": [
    "# Export functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_samples_for_gemini(dataset: Dataset):\n",
    "    print(\"\\n--- EKSPORT PR√ìBEK DLA GEMINI ---\")\n",
    "\n",
    "    sample_range = dataset.select(range(0, 1000))\n",
    "\n",
    "    gemini_data = {\n",
    "        \"info\": \"Polish text samples from CulturaX dataset for regex analysis\",\n",
    "        \"metadata\": {\n",
    "            \"total_samples\": len(sample_range),\n",
    "            \"sample_range\": \"0-999\",  # Zaktualizuj je≈õli zmienisz range\n",
    "            \"export_date\": datetime.now().isoformat(),\n",
    "            \"preprocessing\": [\"HTML tag removal\", \"whitespace normalization\"]\n",
    "        },\n",
    "        \"samples\": []\n",
    "    }\n",
    "\n",
    "    for i, item in enumerate(sample_range):\n",
    "        sample_data = {\n",
    "            \"id\": i + 1,\n",
    "            \"text\": item['text'],\n",
    "            \"length\": len(item['text']),\n",
    "            \"word_count\": len(item['text'].split())\n",
    "        }\n",
    "        gemini_data[\"samples\"].append(sample_data)\n",
    "\n",
    "    output_file = \"samples_for_gemini_full.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(gemini_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Exported {len(sample_range)} samples to {output_file}\")\n",
    "    print(f\"üìä File size: {os.path.getsize(output_file) / 1024:.1f} KB\")\n",
    "    print(f\"üìù Average text length: {sum(s['length'] for s in gemini_data['samples']) / len(gemini_data['samples']):.0f} chars\")\n",
    "\n",
    "    print(f\"\\n--- STATYSTYKI PR√ìBEK ---\")\n",
    "    lengths = [s['length'] for s in gemini_data['samples']]\n",
    "    print(f\"Najkr√≥tszy tekst: {min(lengths)} znak√≥w\")\n",
    "    print(f\"Najd≈Çu≈ºszy tekst: {max(lengths)} znak√≥w\")\n",
    "    print(f\"≈örednia d≈Çugo≈õƒá: {sum(lengths) / len(lengths):.0f} znak√≥w\")\n",
    "\n",
    "    print(f\"\\nüìÅ Plik {output_file} gotowy do wys≈Çania do Gemini!\")\n",
    "    print(f\"üí° U≈ºyj prompt: 'Przeanalizuj te polskie teksty i zaproponuj wyra≈ºenia regularne do ekstrakcji danych'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363d1f5",
   "metadata": {},
   "source": [
    "IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "import re\n",
    "import time\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "    \n",
    "PROMPT_TASKS = {\n",
    "    \"datetimes\": \"Wypisz wszystkie daty i godziny (formaty cyfrowe, np. 2024-10-25, 25.10.2024, 14:30, 08:30:15).\",\n",
    "    \"verbal_dates\": \"Wypisz wszystkie daty zapisane s≈Çownie (np. 10 grudnia 2021, 9 sierpnia, marzec 2020). Zwr√≥ƒá ca≈Çe dopasowanie.\",\n",
    "    \"emails\": \"Wypisz wszystkie adresy e-mail (np. ktos@domena.pl).\",\n",
    "    \"phones\": \"Wypisz wszystkie 9-cyfrowe numery telefon√≥w (polskie formaty, np. 123 456 789, 12-345-67-89, (12) 345 67 89).\",\n",
    "    \"urls\": \"Wypisz wszystkie adresy URL i linki (zaczynajƒÖce siƒô od http, https lub www).\",\n",
    "    \"pln_amounts\": \"Wypisz wszystkie kwoty pieniƒô≈ºne podane w polskiej walucie (np. 18.94 z≈Ç, 1 000 PLN, 500 z≈Çotych). Zwr√≥ƒá kwotƒô wraz z walutƒÖ.\",\n",
    "    \"iban\": \"Wypisz wszystkie numery kont bankowych (polski format NRB, 26 cyfr, np. PL 12 3456... lub 123456...).\",\n",
    "    \"flexion\": \"Wypisz wszystkie formy fleksyjne s≈Ç√≥w 'cz≈Çowiek' i 'ludzie' (np. cz≈Çowiek, cz≈Çowieka, ludziom, lud≈∫mi).\"\n",
    "}\n",
    "\n",
    "def run_llm_extraction(model_name: str, user_prompt: str) -> ExtractionResponse:\n",
    "    \"\"\"Wywo≈Çuje Ollama z timeoutem klienta.\"\"\"\n",
    "    SYSTEM_PROMPT = 'Jeste≈õ precyzyjnym, automatycznym ekstraktorem wzorc√≥w. Zwracasz TYLKO i wy≈ÇƒÖcznie poprawny obiekt JSON w formacie {\"matches\": [...]}. Nie dodawaj ≈ºadnych wyja≈õnie≈Ñ ani markdowna. Bezwzglƒôdnie przestrzegaj REGU≈Å i OGRANICZE≈É podanych przez u≈ºytkownika. Je≈õli w tek≈õcie ABSOLUTNIE nie ma ≈ºadnych dopasowa≈Ñ, zwr√≥ƒá pustƒÖ listƒô: {\"matches\": []}.'\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            format=\"json\",\n",
    "            options={\n",
    "                'temperature': 0 \n",
    "            }\n",
    "        )\n",
    "        json_string = response['message']['content']\n",
    "        data = json.loads(json_string)\n",
    "        validated_response = ExtractionResponse.model_validate(data)\n",
    "        return validated_response\n",
    "    except (json.JSONDecodeError, ValidationError, ollama.ResponseError, Exception) as e:\n",
    "        #print(f\"B≈ÅƒÑD (model: {model_name}, pomijam): {e}\") \n",
    "        return ExtractionResponse(matches=[])\n",
    "\n",
    "def run_llm_on_dataset_sequential(dataset: Dataset, task_key: str, num_samples: int = 100) -> Dataset:\n",
    "    \"\"\"Uruchamia ekstrakcjƒô LLM sekwencyjnie, z separacjƒÖ r√≥l.\"\"\"\n",
    "    if task_key not in PROMPT_TASKS:\n",
    "        raise ValueError(f\"Nieprawid≈Çowy klucz zadania: {task_key}.\")\n",
    "    \n",
    "    print(f\"\\n--- Rozpoczynam SEKWENCYJNƒÑ ekstrakcjƒô LLM (Struktura XML, Temp=0) dla zadania: '{task_key}' ---\")\n",
    "    \n",
    "    subset = dataset.select(range(num_samples))\n",
    "    task_rules_and_constraints = PROMPT_TASKS[task_key] \n",
    "    \n",
    "    col_gemma = f\"gemma2_{task_key}\"\n",
    "    col_llama = f\"llama3_{task_key}\"\n",
    "    \n",
    "    results_gemma = []\n",
    "    results_llama = []\n",
    "\n",
    "    print(f\"Przetwarzanie {len(subset)} pr√≥bek (sekwencyjnie)...\")\n",
    "    for sample in tqdm(subset):\n",
    "        text = sample['text']\n",
    "        \n",
    "        user_prompt = (\n",
    "        f\"{task_rules_and_constraints}\\n\"\n",
    "        f\"\\\"\\\"\\\"\\n\"\n",
    "        f\"{text}\\n\"\n",
    "        f\"\\\"\\\"\\\"\"\n",
    "        )\n",
    "        \n",
    "        gemma_result = run_llm_extraction(\"gemma2:2b\", user_prompt)\n",
    "        llama_result = run_llm_extraction(\"llama3.1:8b\", user_prompt)\n",
    "        \n",
    "        results_gemma.append(gemma_result.matches)\n",
    "        results_llama.append(llama_result.matches)\n",
    "\n",
    "    if col_gemma in subset.column_names:\n",
    "        subset = subset.remove_columns([col_gemma])\n",
    "    if col_llama in subset.column_names:\n",
    "        subset = subset.remove_columns([col_llama])\n",
    "\n",
    "    final_subset = subset.add_column(col_gemma, results_gemma)\n",
    "    final_subset = final_subset.add_column(col_llama, results_llama)\n",
    "    \n",
    "    print(f\"Przetwarzanie '{task_key}' zako≈Ñczone.\")\n",
    "    return final_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6f8e4",
   "metadata": {},
   "source": [
    "Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52144f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def export_text_batches(dataset, batch_size=100, out_dir=\"batches\", prefix=\"batch\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    total = len(dataset)\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        try:\n",
    "            texts = dataset.select(range(start, end))['text']\n",
    "        except Exception:\n",
    "            texts = [dataset[i]['text'] for i in range(start, end)]\n",
    "        records = [{\"text\": t} for t in texts]\n",
    "        fname = os.path.join(out_dir, f\"{prefix}_{start//batch_size + 1}.json\")\n",
    "        with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Wrote {len(records)} records -> {fname}\")\n",
    "\n",
    "export_text_batches(ds_final_analysis, batch_size=100, out_dir=\"batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "def merge_batches_with_golden(batch_dir=\"batches\",\n",
    "                              golden_dir=\"golden_batches\",\n",
    "                              merged_dir=\"merged\",\n",
    "                              batch_prefix=\"batch\",\n",
    "                              golden_prefix=\"golden_batch\"):\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "\n",
    "    idx_re = re.compile(rf\"{re.escape(batch_prefix)}_(\\d+)\\.json$\")\n",
    "    batch_files = sorted(\n",
    "        [f for f in os.listdir(batch_dir) if idx_re.match(f)],\n",
    "        key=lambda x: int(idx_re.match(x).group(1))\n",
    "    )\n",
    "\n",
    "    if not batch_files:\n",
    "        print(\"Nie znaleziono plik√≥w batch w:\", batch_dir)\n",
    "        return\n",
    "\n",
    "    for bf in batch_files:\n",
    "        idx = idx_re.match(bf).group(1)\n",
    "        batch_path = os.path.join(batch_dir, bf)\n",
    "        golden_fname = f\"{golden_prefix}_{idx}.json\"\n",
    "        golden_path = os.path.join(golden_dir, golden_fname)\n",
    "\n",
    "        with open(batch_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            batch_records = json.load(f)\n",
    "\n",
    "        if os.path.exists(golden_path):\n",
    "            with open(golden_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                golden_records = json.load(f)\n",
    "        else:\n",
    "            golden_records = []\n",
    "\n",
    "        max_len = max(len(batch_records), len(golden_records))\n",
    "        merged_records = []\n",
    "        for i in range(max_len):\n",
    "            rec_batch = batch_records[i] if i < len(batch_records) else {}\n",
    "            rec_golden = golden_records[i] if i < len(golden_records) else {}\n",
    "            merged = dict(rec_batch)\n",
    "\n",
    "            if rec_golden:\n",
    "                merged[\"golden\"] = rec_golden\n",
    "            merged_records.append(merged)\n",
    "\n",
    "        out_fname = os.path.join(merged_dir, f\"merged_{batch_prefix}_{idx}.json\")\n",
    "        with open(out_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Merged {len(merged_records)} records -> {out_fname}\")\n",
    "\n",
    "merge_batches_with_golden(batch_dir=\"batches\", golden_dir=\"golden_batches\", merged_dir=\"merged\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
